"""
tiktokenì„ ì‚¬ìš©í•œ í† í° ì¹´ìš´í„° ì‚¬ìš© ì˜ˆì œ

ì´ ì˜ˆì œëŠ” token_counter.py ëª¨ë“ˆì˜ ê¸°ë³¸ ì‚¬ìš©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
"""

from token_counter import (
    count_tokens_from_directory,
    count_tokens_from_file,
    count_tokens_from_text,
    print_token_analysis,
)


def example_text_token_count():
    """í…ìŠ¤íŠ¸ì—ì„œ ì§ì ‘ í† í° ìˆ˜ ê³„ì‚° ì˜ˆì œ"""
    print("=" * 50)
    print("ğŸ“ í…ìŠ¤íŠ¸ í† í° ê³„ì‚° ì˜ˆì œ")
    print("=" * 50)

    # í•œê¸€ í…ìŠ¤íŠ¸ ì˜ˆì œ
    korean_text = """
    ì£¼ì„±ì—”ì§€ë‹ˆì–´ë§ì€ ë°˜ë„ì²´ ë° ë””ìŠ¤í”Œë ˆì´ ì œì¡° ì¥ë¹„ë¥¼ ìƒì‚°í•˜ëŠ” íšŒì‚¬ì…ë‹ˆë‹¤.
    1993ë…„ì— ì„¤ë¦½ë˜ì–´ ì§€ì†ì ì¸ ê¸°ìˆ  ê°œë°œê³¼ í˜ì‹ ì„ í†µí•´ ì„±ì¥í•´ì™”ìŠµë‹ˆë‹¤.
    """

    # ì˜ì–´ í…ìŠ¤íŠ¸ ì˜ˆì œ
    english_text = """
    JUSUNG Engineering is a company that manufactures semiconductor and display equipment.
    Founded in 1993, the company has grown through continuous technological development and innovation.
    """

    # HTML í…ìŠ¤íŠ¸ ì˜ˆì œ
    html_text = """
    <html>
    <head><title>ì£¼ì„±ì—”ì§€ë‹ˆì–´ë§</title></head>
    <body>
        <h1>ì£¼ì„±ì—”ì§€ë‹ˆì–´ë§ ì£¼ì‹íšŒì‚¬</h1>
        <p>ë°˜ë„ì²´ ì œì¡° ì¥ë¹„ ì „ë¬¸ ê¸°ì—…</p>
        <table>
            <tr><td>ì„¤ë¦½ë…„ë„</td><td>1993</td></tr>
            <tr><td>ì—…ì¢…</td><td>ë°˜ë„ì²´ ì¥ë¹„</td></tr>
        </table>
    </body>
    </html>
    """

    # ê° í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ ê³„ì‚°
    texts = [
        ("í•œê¸€ í…ìŠ¤íŠ¸", korean_text),
        ("ì˜ì–´ í…ìŠ¤íŠ¸", english_text),
        ("HTML í…ìŠ¤íŠ¸", html_text),
    ]

    for name, text in texts:
        token_count = count_tokens_from_text(text.strip())
        char_count = len(text.strip())
        print(f"\nğŸ“„ {name}:")
        print(f"   ë¬¸ì ìˆ˜: {char_count:,}")
        print(f"   í† í° ìˆ˜: {token_count:,}")
        print(f"   í† í°/ë¬¸ì ë¹„ìœ¨: {token_count / char_count:.4f}")


def example_file_token_count():
    """íŒŒì¼ì—ì„œ í† í° ìˆ˜ ê³„ì‚° ì˜ˆì œ"""
    print("\n" + "=" * 50)
    print("ğŸ“ íŒŒì¼ í† í° ê³„ì‚° ì˜ˆì œ")
    print("=" * 50)

    # ì£¼ì„±ì—”ì§€ë‹ˆì–´ë§.html íŒŒì¼ ë¶„ì„
    html_file = "ì£¼ì„±ì—”ì§€ë‹ˆì–´ë§.html"
    try:
        result = count_tokens_from_file(html_file)
        print_token_analysis(result)

        # ì¶”ê°€ ë¶„ì„ ì •ë³´
        if result["status"] == "success":
            file_size_kb = result["file_size_bytes"] / 1024
            estimated_cost_gpt4 = (
                result["token_count"] * 0.03 / 1000
            )  # GPT-4 input cost
            estimated_cost_gpt35 = (
                result["token_count"] * 0.001 / 1000
            )  # GPT-3.5 input cost

            print("\nğŸ’° ì˜ˆìƒ API ë¹„ìš© (USD):")
            print(f"   GPT-4: ${estimated_cost_gpt4:.6f}")
            print(f"   GPT-3.5-turbo: ${estimated_cost_gpt35:.6f}")

            print("\nğŸ“Š ì¶”ê°€ í†µê³„:")
            print(f"   íŒŒì¼ í¬ê¸°: {file_size_kb:.1f} KB")
            print(
                f"   ì••ì¶•ë¥  ì¶”ì •: {result['token_count'] / result['character_count']:.4f}"
            )

    except Exception as e:
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")


def example_directory_token_count():
    """ë””ë ‰í† ë¦¬ ì „ì²´ í† í° ìˆ˜ ê³„ì‚° ì˜ˆì œ"""
    print("\n" + "=" * 50)
    print("ğŸ“‚ ë””ë ‰í† ë¦¬ í† í° ê³„ì‚° ì˜ˆì œ")
    print("=" * 50)

    # í˜„ì¬ ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  HTML íŒŒì¼ ë¶„ì„
    results = count_tokens_from_directory(".", [".html"])

    if results:
        total_tokens = 0
        total_files = 0

        for result in results:
            print_token_analysis(result)
            if result["status"] == "success":
                total_tokens += result["token_count"]
                total_files += 1

        if total_files > 0:
            print("\nğŸ¯ ì „ì²´ ìš”ì•½:")
            print(f"   ì´ {total_files}ê°œ íŒŒì¼")
            print(f"   ì´ {total_tokens:,} í† í°")
            print(f"   í‰ê·  {total_tokens // total_files:,} í† í°/íŒŒì¼")
    else:
        print("ğŸ“­ ë¶„ì„í•  HTML íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")


def example_model_comparison():
    """ë‹¤ì–‘í•œ ëª¨ë¸ì—ì„œì˜ í† í° ìˆ˜ ë¹„êµ ì˜ˆì œ"""
    print("\n" + "=" * 50)
    print("ğŸ”„ ëª¨ë¸ë³„ í† í° ìˆ˜ ë¹„êµ ì˜ˆì œ")
    print("=" * 50)

    sample_text = """
    ì£¼ì„±ì—”ì§€ë‹ˆì–´ë§(ì£¼)ì€ ë°˜ë„ì²´ ë° ë””ìŠ¤í”Œë ˆì´ ì œì¡° ì¥ë¹„ë¥¼ ê°œë°œÂ·ìƒì‚°í•˜ëŠ” ì „ë¬¸ê¸°ì—…ì…ë‹ˆë‹¤.
    1993ë…„ ì„¤ë¦½ ì´ë˜ ì§€ì†ì ì¸ ì—°êµ¬ê°œë°œì„ í†µí•´ ì„¸ê³„ ìµœê³  ìˆ˜ì¤€ì˜ ê¸°ìˆ ë ¥ì„ í™•ë³´í•˜ê³  ìˆìŠµë‹ˆë‹¤.
    íŠ¹íˆ CVD(Chemical Vapor Deposition) ì¥ë¹„ ë¶„ì•¼ì—ì„œ ì„¸ê³„ì ì¸ ê²½ìŸë ¥ì„ ë³´ìœ í•˜ê³  ìˆìŠµë‹ˆë‹¤.
    """

    models = ["gpt-4", "gpt-3.5-turbo"]

    print(f"ğŸ“ ë¶„ì„ í…ìŠ¤íŠ¸: {len(sample_text)} ë¬¸ì")
    print("-" * 50)

    for model in models:
        try:
            token_count = count_tokens_from_text(sample_text, model)
            print(f"ğŸ¤– {model:15}: {token_count:,} í† í°")
        except Exception as e:
            print(f"âŒ {model:15}: ì˜¤ë¥˜ - {e}")


if __name__ == "__main__":
    print("ğŸš€ tiktoken í† í° ì¹´ìš´í„° ì˜ˆì œ ì‹¤í–‰")

    # 1. í…ìŠ¤íŠ¸ í† í° ê³„ì‚°
    example_text_token_count()

    # 2. íŒŒì¼ í† í° ê³„ì‚°
    example_file_token_count()

    # 3. ë””ë ‰í† ë¦¬ í† í° ê³„ì‚°
    example_directory_token_count()

    # 4. ëª¨ë¸ë³„ ë¹„êµ
    example_model_comparison()

    print("\n" + "=" * 50)
    print("âœ… ëª¨ë“  ì˜ˆì œ ì™„ë£Œ!")
    print("=" * 50)
