{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b835ccc3",
   "metadata": {},
   "source": [
    "## 1-4-2. LangChain의 LLM 모델 파라미터 설정\n",
    "\n",
    "\n",
    "LLM 모델의 기본 속성값을 조정하는 방법에 대해서 살펴봅니다. 모델의 속성에 해당하는 모델 파라미터는 LLM의 출력을 조정하고 최적화하는데 사용되며, 모델이 생성하는 텍스트의 스타일, 길이, 정확도 등에 영향을 주게 됩니다. 사용하는 모델이나 플랫폼에 따라 세부 내용은 차이가 있습니다.\n",
    "\n",
    "일반적으로 적용되는 주요 파라미터는 다음과 같습니다.\n",
    "\n",
    "- Temperature: 생성된 텍스트의 다양성을 조정합니다. 값이 작으면 예측 가능하고 일관된 출력을 생성하는 반면, 값이 크면 다양하고 예측하기 어려운 출력을 생성합니다.\n",
    "\n",
    "- Max Tokens (최대 토큰 수): 생성할 최대 토큰 수를 지정합니다. 생성할 텍스트의 길이를 제한합니다.\n",
    "\n",
    "- Top P (Top Probability): 생성 과정에서 특정 확률 분포 내에서 상위 P% 토큰만을 고려하는 방식입니다. 이는 출력의 다양성을 조정하는 데 도움이 됩니다.\n",
    "\n",
    "- Frequency Penalty (빈도 패널티): 값이 클수록 이미 등장한 단어나 구절이 다시 등장할 확률을 감소시킵니다. 이를 통해 반복을 줄이고 텍스트의 다양성을 증가시킬 수 있습니다. (0~1)\n",
    "\n",
    "- Presence Penalty (존재 패널티): 텍스트 내에서 단어의 존재 유무에 따라 그 단어의 선택 확률을 조정합니다. 값이 클수록 아직 텍스트에 등장하지 않은 새로운 단어의 사용이 장려됩니다. (0~1)\n",
    "\n",
    "- Stop Sequences (정지 시퀀스): 특정 단어나 구절이 등장할 경우 생성을 멈추도록 설정합니다. 이는 출력을 특정 포인트에서 종료하고자 할 때 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17d91bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bigseoul/workspace/indgram/.venv/lib/python3.14/site-packages/IPython/core/interactiveshell.py:3639: UserWarning: Parameters {'stop', 'presence_penalty', 'frequency_penalty'} should be specified explicitly. Instead they were passed in as part of `model_kwargs` parameter.\n",
      "  if await self.run_code(code, result, async_=asy):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='태양계에서 가장 큰 행성은 목성(Jupiter)입니다. 목성은 지구의 약 11배 크고, 직경은 약 142,984킬로미터에 달합니다. 또한, 목성은 가스 거인이며, 두꺼운 대기와 강력한 자기장을 가지고 있습니다.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 70, 'prompt_tokens': 20, 'total_tokens': 90, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_c4585b5b9c', 'id': 'chatcmpl-CxqXY3H88LpFUlZlELYM2P1crs4MV', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--019bbb9f-8ff6-7472-9897-7b8b0e052bdb-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 20, 'output_tokens': 70, 'total_tokens': 90, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 모델 파라미터 설정\n",
    "params = {\n",
    "    \"temperature\": 0.7,         # 생성된 텍스트의 다양성 조정\n",
    "    \"max_tokens\": 100,          # 생성할 최대 토큰 수    \n",
    "}\n",
    "\n",
    "kwargs = {\n",
    "    \"frequency_penalty\": 0.5,   # 이미 등장한 단어의 재등장 확률\n",
    "    \"presence_penalty\": 0.5,    # 새로운 단어의 도입을 장려\n",
    "    \"stop\": [\"\\n\"]              # 정지 시퀀스 설정\n",
    "\n",
    "}\n",
    "\n",
    "# 모델 인스턴스를 생성할 때 설정\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", **params, model_kwargs = kwargs)\n",
    "\n",
    "\n",
    "# 모델 호출\n",
    "question = \"태양계에서 가장 큰 행성은 무엇인가요?\"\n",
    "response = model.invoke(input=question)\n",
    "\n",
    "# 전체 응답 출력\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df507ec4",
   "metadata": {},
   "source": [
    "모델 호출 단계\n",
    "다음은 앞에서 생성한 모델 인스턴스를 이용하여, invoke 메소드를 사용하여 새로운 호출을 할 때 모델의 기본 파라미터(params)를 설정하는 방법입니다. 실행 결과를 보면 최대 10 토큰의 길이로 답변이 생성됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9ad2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 파라미터 설정\n",
    "params = {\n",
    "    \"temperature\": 0.7,         # 생성된 텍스트의 다양성 조정\n",
    "    \"max_tokens\": 10,          # 생성할 최대 토큰 수    \n",
    "}\n",
    "\n",
    "# 모델 인스턴스를 호출할 때 전달\n",
    "response = model.invoke(input=question, **params)\n",
    "\n",
    "# 문자열 출력\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92b6b6c",
   "metadata": {},
   "source": [
    "1-4-2-2. LLM 모델 파라미터를 추가로 바인딩 (bind 메소드)\n",
    "\n",
    "bind 메소드를 사용하여 모델 인스턴스에 파라미터를 추가로 제공할 수 있습니다. bind 메서드를 사용하는 방식의 장점은 특정 모델 설정을 기본값으로 사용하고자 할 때 유용하며, 특수한 상황에서 일부 파라미터를 다르게 적용하고 싶을 때 사용합니다. 기본적으로 일관된 파라미터 설정을 유지하면서 상황에 맞춰 유연한 대응이 가능합니다. 이를 통해 코드의 가독성과 재사용성을 높일 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e787d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='태양계에서 가장 큰 행성은 목성(Jupiter)입니다. 목성은 지름이 약 142,984킬로미터에 달하며, 태양계의 다른 모든 행성을 합친 것보다도 더 큰 질량을 가지고 있습니다.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 56, 'prompt_tokens': 38, 'total_tokens': 94, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_c4585b5b9c', 'id': 'chatcmpl-CxqbdweUChIRLAkpaZBCUPiKyHvuT', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--019bbba3-6031-7180-925c-865d0733309a-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 38, 'output_tokens': 56, 'total_tokens': 94, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "content='태양계에서 가장 큰 행성은 목' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 38, 'total_tokens': 48, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_c4585b5b9c', 'id': 'chatcmpl-Cxqbf8hwj7Xf1Pp9NyEr5qMXb0WID', 'service_tier': 'default', 'finish_reason': 'length', 'logprobs': None} id='lc_run--019bbba3-67cb-7203-bbeb-172f2fe340c7-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 38, 'output_tokens': 10, 'total_tokens': 48, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"이 시스템은 천문학 질문에 답변할 수 있습니다.\"),\n",
    "    (\"user\", \"{user_input}\"),\n",
    "])\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", max_tokens=100)\n",
    "\n",
    "messages = prompt.format_messages(user_input=\"태양계에서 가장 큰 행성은 무엇인가요?\")\n",
    "\n",
    "before_answer = model.invoke(messages)\n",
    "\n",
    "# # binding 이전 출력\n",
    "print(before_answer)\n",
    "\n",
    "# 모델 호출 시 추가적인 인수를 전달하기 위해 bind 메서드 사용 (응답의 최대 길이를 10 토큰으로 제한)\n",
    "chain = prompt | model.bind(max_tokens=10)\n",
    "\n",
    "after_answer = chain.invoke({\"user_input\": \"태양계에서 가장 큰 행성은 무엇인가요?\"})\n",
    "\n",
    "# binding 이후 출력\n",
    "print(after_answer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
