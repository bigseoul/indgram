import os
import re
import sys
from pathlib import Path
from typing import Dict

from bs4 import BeautifulSoup

SAMPLE = "íˆ¬ë¯¹ìŠ¤í™€ë”©ìŠ¤.html"

# ì£¼ì„ ë° footnote íŒ¨í„´: (*1), (ì£¼1), *1, [1], ì£¼1 ë“± ëŒ€ì‘
FOOTNOTE_PATTERN = re.compile(
    r"\(\s*[\*ì£¼]?\s*\d+\s*\)|"  # (1), (*1), (ì£¼1)
    r"\[\s*[\*ì£¼]?\s*\d+\s*\]|"  # [1], [*1]
    r"[\*ì£¼]\d+|"  # *1, ì£¼1
    r"^\s*[\*ì£¼]\s*$"  # ë‹¨ë… * ë˜ëŠ” ì£¼
)


def clear_terminal():
    os.system("cls" if os.name == "nt" else "clear")


def _clean_text(text: str, is_navigable_string: bool = False) -> str:
    if not text:
        return ""
    cleaned = FOOTNOTE_PATTERN.sub("", text)
    if not cleaned.strip():
        return " " if text.strip() or is_navigable_string else ""
    if is_navigable_string:
        return re.sub(r"\s+", " ", cleaned)
    return cleaned.strip()


def _get_simplified_html(tag) -> str:
    from bs4 import NavigableString, Tag

    if isinstance(tag, NavigableString):
        return _clean_text(str(tag), is_navigable_string=True)
    if not isinstance(tag, Tag):
        return ""
    raw_tag_text = tag.get_text()
    if raw_tag_text.strip() and not _clean_text(raw_tag_text).strip():
        return ""
    inner_html = "".join(_get_simplified_html(child) for child in tag.children)
    inner_html = re.sub(r"\s+", " ", inner_html).strip()
    if not inner_html and tag.name not in ["td", "th", "tr"]:
        return ""
    attrs_str = ""
    if tag.name in ["td", "th"]:
        for attr in ["colspan", "rowspan"]:
            if tag.has_attr(attr):
                attrs_str += f' {attr}="{tag[attr]}"'
    return f"<{tag.name}{attrs_str}>{inner_html}</{tag.name}>"


def _normalize(text: str) -> str:
    if not text:
        return ""
    return re.sub(r"[\s\xa0]+", " ", text).strip()


def _get_company_name(soup: BeautifulSoup) -> str:
    title = soup.title.string if soup.title else ""
    if title:
        match = re.search(
            r"([ê°€-í£a-zA-Z0-9]+?\s*ì£¼ì‹íšŒì‚¬|ì£¼ì‹íšŒì‚¬\s*[ê°€-í£a-zA-Z0-9]+?)", title
        )
        if match:
            return _normalize(match.group(0))
    first_p = soup.find("p")
    if first_p:
        text = first_p.get_text()
        match = re.search(
            r"([ê°€-í£a-zA-Z0-9]+?\s*ì£¼ì‹íšŒì‚¬|ì£¼ì‹íšŒì‚¬\s*[ê°€-í£a-zA-Z0-9]+?)", text
        )
        if match:
            return _normalize(match.group(0))
    return "Unknown Company"


def _extract_global_meta(soup: BeautifulSoup) -> Dict:
    meta = {"unit": "ì›", "as_of_date": "Unknown"}
    text = soup.get_text()
    unit_match = re.search(r"\(ë‹¨ìœ„\s*:\s*([ê°€-í£]+)\)", text)
    if unit_match:
        meta["unit"] = unit_match.group(1)
    date_patterns = [
        r"(\d{4}ë…„\s*\d{1,2}ì›”\s*\d{1,2}ì¼)\s*í˜„ì¬",
        r"ì œ\s*\d+\s*ê¸°ë§\s*(\d{4}ë…„\s*\d{1,2}ì›”\s*\d{1,2}ì¼)",
    ]
    for pattern in date_patterns:
        match = re.search(pattern, text)
        if match:
            meta["as_of_date"] = match.group(1)
            break
    return meta


def extract_evidence_blocks(soup: BeautifulSoup) -> str:
    """
    HTML ì›ë³¸ ì¶”ì¶œê¸° (v4):
    1. ì¸íŠ¸ë¡œ ì„¹ì…˜(íšŒì‚¬ì˜ ê°œìš” ë“±)ì€ í…ìŠ¤íŠ¸ì™€ í…Œì´ë¸” ëª¨ë‘ ì¶”ì¶œ
    2. ê·¸ ì™¸ ì„¹ì…˜ì€ 'ì§€ë¶„/ì£¼ì£¼' í‚¤ì›Œë“œì™€ '%' ê¸°í˜¸ê°€ í•¨ê»˜ ìˆëŠ” í…Œì´ë¸”/í…ìŠ¤íŠ¸ ìœ„ì£¼ë¡œ ì¶”ì¶œ
    3. íšŒê³„ ì›ì¹™ ìœ„ì£¼ì˜ ì´ë¡ ì  ë¬¸ì¥ì€ í•„í„°ë§
    """
    company_name = _get_company_name(soup)
    global_meta = _extract_global_meta(soup)

    section_pattern = re.compile(
        r"^\s*([0-9]{1,2}|[IVX]{1,3}|[ê°€-í•˜])[\.\)\s]+\s*(íšŒì‚¬ì˜\s*ê°œìš”|ì¼ë°˜ì‚¬í•­|ì¼ë°˜ì ì¸\s*ì‚¬í•­|ì¼ë°˜\s*ì‚¬í•­)",
        re.IGNORECASE,
    )
    any_section_pattern = re.compile(
        r"^\s*([0-9]{1,2}|[IVX]{1,3}|[ê°€-í•˜])[\.\)\s]+\s*([ê°€-í£\s]{2,50})"
    )

    skip_section_keywords = [
        r"íšŒê³„ì •ì±…",
        r"ì‘ì„±ê¸°ì¤€",
        r"í˜„ê¸ˆíë¦„í‘œ",
        r"ì£¼ë‹¹ì†ìµ",
        r"ì£¼ë‹¹ìˆœì´ìµ",
        r"ìœ„í—˜ê´€ë¦¬",
        r"ê¸ˆìœµìƒí’ˆì˜\s*ë²”ì£¼",
        r"ê¸ˆìœµìì‚°",
        r"ê¸ˆìœµë¶€ì±„",
        r"ìš°ë°œì±„ë¬´",
        r"ì•½ì •ì‚¬í•­",
    ]

    accounting_principle_pattern = re.compile(
        r"(ì¸ì‹|ì¸¡ì •|ì²˜ë¦¬|ê³„ìƒ|ë¶„ë¥˜|ì ìš©)(í•©ë‹ˆë‹¤|ë©ë‹ˆë‹¤|í•˜ë©°|í•˜ì—¬|í•˜ë˜)\.?\s*$",
        re.MULTILINE,
    )

    data_keywords = [
        "ì§€ë¶„",
        "ì£¼ì£¼",
        "ìë³¸ê¸ˆ",
        "ì¶œì",
        "ì¢…ì†ê¸°ì—…",
        "í”¼íˆ¬ì",
        "ì†Œìœ ",
        "ë³´ìœ ",
        "ì§€ë°°",
    ]

    # ì§€ë¶„ìœ¨ ë°ì´í„°ì„ì„ í™•ì‹ í•˜ê²Œ í•´ì£¼ëŠ” ê°•ë ¥í•œ ë¬¸ì
    ratio_marker = "%"

    exclude_keywords = [
        "ë¹„ì§€ë°°ì§€ë¶„ìœ¨",
        "ë¹„ì§€ë°°ì§€ë¶„",
        "ì±„ê¶Œ",
        "ì±„ë¬´",
        "ë§¤ì¶œ",
        "ë§¤ì…",
        "ì§€ê¸‰ë³´ì¦",
        "ë‹´ë³´ì œê³µ",
        "ì£¼ìš”ê±°ë˜",
        "ìê¸ˆê±°ë˜",
        "ìˆ˜ìµ",
        "ë¹„ìš©",
        "ì±„ë¬´ë©´ì œ",
    ]

    header = [
        "[META]",
        f"Company: {company_name}",
        f"Unit: {global_meta.get('unit', 'Unknown')}",
        f"Date: {global_meta.get('as_of_date', 'Unknown')}",
    ]

    evidence = ["\n".join(header)]
    seen_elements = set()
    all_tags = soup.find_all(["h1", "h2", "h3", "p", "table", "div", "span"])

    idx = 0
    current_section = "Unknown Section"
    while idx < len(all_tags):
        tag = all_tags[idx]
        if tag in seen_elements:
            idx += 1
            continue

        raw_text = tag.get_text().strip()
        if not raw_text:
            idx += 1
            continue

        if tag.name in ["h1", "h2", "h3"] or (tag.name == "p" and len(raw_text) < 100):
            first_line = raw_text.split("\n")[0].strip()
            section_m = any_section_pattern.match(first_line)
            if section_m:
                current_section = first_line

        if any(re.search(kw, current_section) for kw in skip_section_keywords):
            seen_elements.add(tag)
            idx += 1
            continue

        is_intro_section = bool(section_pattern.match(current_section))

        if not _clean_text(raw_text).strip():
            seen_elements.add(tag)
            idx += 1
            continue

        # [v4] í…Œì´ë¸” ì¶”ì¶œ ë¡œì§ ê°•í™”
        if tag.name == "table":
            table_text = tag.get_text()

            # ì¸íŠ¸ë¡œ ì„¹ì…˜ì´ë©´ ë¬´ì¡°ê±´ ê°€ì ¸ê°, ê·¸ ì™¸ì—ëŠ” ì§€ë¶„ í‚¤ì›Œë“œì™€ %ê°€ ìˆì–´ì•¼ í•¨
            has_ratio = ratio_marker in table_text or "ì§€ë¶„ìœ¨" in table_text
            has_keyword = any(kw in table_text for kw in data_keywords)
            not_excluded = not any(ek in table_text for ek in exclude_keywords)

            if is_intro_section or (has_ratio and has_keyword and not_excluded):
                evidence.append(
                    f"[DATA-TABLE-HTML]\n[Section: {current_section}]\n{_get_simplified_html(tag)}"
                )
                seen_elements.add(tag)
                for desc in tag.find_all(True):
                    seen_elements.add(desc)
            else:
                seen_elements.add(tag)
            idx += 1
            continue

        # [v4] í…ìŠ¤íŠ¸ ë¸”ë¡(P ë“±) ì¶”ì¶œ ë¡œì§
        if is_intro_section:
            # ì¸íŠ¸ë¡œ ì„¹ì…˜ì€ ì¼ë°˜ì ì¸ ê²½ìš° ë‹¤ ê°€ì ¸ì˜´ (ë‹¨, ë‹¤ë¥¸ ì„¹ì…˜ ì‹œì‘ ì „ê¹Œì§€)
            if section_pattern.match(raw_text):
                block_content = []
                curr_idx = idx
                count = 0
                while curr_idx < len(all_tags) and count < 15:
                    t = all_tags[curr_idx]
                    if count > 0 and (
                        t.name in ["h1", "h2", "h3"]
                        or section_pattern.match(t.get_text().strip())
                    ):
                        break
                    if t not in seen_elements:
                        if _clean_text(t.get_text()).strip():
                            block_content.append(_get_simplified_html(t))
                            seen_elements.add(t)
                            if hasattr(t, "find_all"):
                                for desc in t.find_all(True):
                                    seen_elements.add(desc)
                    curr_idx += 1
                    count += 1
                if block_content:
                    evidence.append(
                        f"[DATA-GENERAL-HTML]\n[Section: {current_section}]\n"
                        + "\n".join(block_content)
                    )
                idx = curr_idx
                continue
            else:
                evidence.append(
                    f"[DATA-GENERAL-HTML]\n[Section: {current_section}]\n{_get_simplified_html(tag)}"
                )
                seen_elements.add(tag)
        else:
            # ê·¸ ì™¸ ì„¹ì…˜ì˜ í…ìŠ¤íŠ¸: íšŒê³„ ì›ì¹™ì´ ì•„ë‹ˆë©´ì„œ + ì§€ë¶„ í‚¤ì›Œë“œì™€ %ê°€ ê°™ì´ ìˆëŠ” ê²½ìš°ë§Œ!
            is_principle = bool(accounting_principle_pattern.search(raw_text))
            has_keyword = any(kw in raw_text for kw in data_keywords)
            has_ratio = ratio_marker in raw_text

            if not is_principle and has_keyword and has_ratio:
                evidence.append(
                    f"[DATA-GENERAL-HTML]\n[Section: {current_section}]\n{_get_simplified_html(tag)}"
                )
                seen_elements.add(tag)

        idx += 1

    return "\n\n---\n\n".join(evidence)


if __name__ == "__main__":
    sys.path.append(str(Path(__file__).resolve().parent.parent.parent / "tokenizer"))
    try:
        from token_counter import count_tokens_from_text, count_tokens_gemini
    except ImportError:
        count_tokens_from_text = None
        count_tokens_gemini = None

    test_file = Path(__file__).resolve().parent / "sample" / SAMPLE
    if not test_file.exists():
        print(f"Error: {test_file} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    else:
        content = test_file.read_text(encoding="utf-8")
        soup = BeautifulSoup(content, "html.parser")
        result = extract_evidence_blocks(soup)
        clear_terminal()
        print(result)
        if count_tokens_from_text:
            tokens_gpt = count_tokens_from_text(result, model_name="gpt-5-nano")
            tokens_gemini = (
                count_tokens_gemini(result) if count_tokens_gemini else "N/A"
            )
            print("\n" + "=" * 50)
            print("ğŸ“Š Token Analysis (Extracted Content):")
            print(f"   Characters: {len(result):,}")
            print(f"   GPT Tokens: {tokens_gpt:,}")
            print(
                f"   Gemini Tokens: {tokens_gemini if isinstance(tokens_gemini, str) else f'{tokens_gemini:,}'}"
            )
            print("=" * 50)
