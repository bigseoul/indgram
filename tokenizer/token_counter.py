import argparse
import os
from pathlib import Path
from typing import Dict, List

import tiktoken


def get_encoding(model_name: str = "gpt-4") -> tiktoken.Encoding:
    """
    tiktoken ì¸ì½”ë”© ê°ì²´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

    Args:
        model_name: ì‚¬ìš©í•  ëª¨ë¸ëª… (ê¸°ë³¸ê°’: "gpt-4")

    Returns:
        tiktoken.Encoding: ì¸ì½”ë”© ê°ì²´
    """
    try:
        encoding = tiktoken.encoding_for_model(model_name)
    except KeyError:
        # ëª¨ë¸ëª…ì´ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²½ìš° ê¸°ë³¸ ì¸ì½”ë”© ì‚¬ìš©
        encoding = tiktoken.get_encoding("cl100k_base")

    return encoding


def count_tokens_from_text(text: str, model_name: str = "gpt-4") -> int:
    """
    í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.

    Args:
        text: í† í°ì„ ê³„ì‚°í•  í…ìŠ¤íŠ¸
        model_name: ì‚¬ìš©í•  ëª¨ë¸ëª…

    Returns:
        int: í† í° ìˆ˜
    """
    encoding = get_encoding(model_name)
    tokens = encoding.encode(text)
    return len(tokens)


def count_tokens_from_file(
    file_path: str, model_name: str = "gpt-4", encoding: str = "utf-8"
) -> Dict[str, any]:
    """
    íŒŒì¼ì—ì„œ í† í° ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.

    Args:
        file_path: íŒŒì¼ ê²½ë¡œ
        model_name: ì‚¬ìš©í•  ëª¨ë¸ëª…
        encoding: íŒŒì¼ ì¸ì½”ë”©

    Returns:
        Dict: íŒŒì¼ ì •ë³´ì™€ í† í° ìˆ˜ê°€ í¬í•¨ëœ ë”•ì…”ë„ˆë¦¬
    """
    try:
        with open(file_path, "r", encoding=encoding) as file:
            content = file.read()

        token_count = count_tokens_from_text(content, model_name)
        file_size = os.path.getsize(file_path)

        return {
            "file_path": file_path,
            "file_size_bytes": file_size,
            "character_count": len(content),
            "token_count": token_count,
            "tokens_per_byte": token_count / file_size if file_size > 0 else 0,
            "model_used": model_name,
            "status": "success",
        }

    except UnicodeDecodeError:
        # UTF-8ë¡œ ì½ì„ ìˆ˜ ì—†ëŠ” ê²½ìš° ë‹¤ë¥¸ ì¸ì½”ë”© ì‹œë„
        try:
            with open(file_path, "r", encoding="cp949") as file:
                content = file.read()
            token_count = count_tokens_from_text(content, model_name)
            file_size = os.path.getsize(file_path)

            return {
                "file_path": file_path,
                "file_size_bytes": file_size,
                "character_count": len(content),
                "token_count": token_count,
                "tokens_per_byte": token_count / file_size if file_size > 0 else 0,
                "model_used": model_name,
                "encoding_used": "cp949",
                "status": "success",
            }
        except Exception as e:
            return {"file_path": file_path, "error": str(e), "status": "error"}

    except Exception as e:
        return {"file_path": file_path, "error": str(e), "status": "error"}


def count_tokens_from_directory(
    directory_path: str, file_extensions: List[str] = None, model_name: str = "gpt-4"
) -> List[Dict[str, any]]:
    """
    ë””ë ‰í† ë¦¬ ë‚´ì˜ ëª¨ë“  íŒŒì¼ì—ì„œ í† í° ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.

    Args:
        directory_path: ë””ë ‰í† ë¦¬ ê²½ë¡œ
        file_extensions: ì²˜ë¦¬í•  íŒŒì¼ í™•ì¥ì ë¦¬ìŠ¤íŠ¸ (ê¸°ë³¸ê°’: ['.html', '.txt', '.md'])
        model_name: ì‚¬ìš©í•  ëª¨ë¸ëª…

    Returns:
        List[Dict]: ê° íŒŒì¼ì˜ í† í° ì •ë³´ê°€ í¬í•¨ëœ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸
    """
    if file_extensions is None:
        file_extensions = [".html", ".txt", ".md", ".py", ".js", ".css"]

    results = []
    directory = Path(directory_path)

    for file_path in directory.rglob("*"):
        if file_path.is_file() and file_path.suffix.lower() in file_extensions:
            result = count_tokens_from_file(str(file_path), model_name)
            results.append(result)

    return results


def print_token_analysis(result: Dict[str, any]):
    """í† í° ë¶„ì„ ê²°ê³¼ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤."""
    if result["status"] == "success":
        print(f"\nğŸ“„ íŒŒì¼: {result['file_path']}")
        print(f"   íŒŒì¼ í¬ê¸°: {result['file_size_bytes']:,} bytes")
        print(f"   ë¬¸ì ìˆ˜: {result['character_count']:,} characters")
        print(f"   í† í° ìˆ˜: {result['token_count']:,} tokens")
        print(f"   í† í°/ë°”ì´íŠ¸ ë¹„ìœ¨: {result['tokens_per_byte']:.4f}")
        print(f"   ì‚¬ìš© ëª¨ë¸: {result['model_used']}")
        if "encoding_used" in result:
            print(f"   íŒŒì¼ ì¸ì½”ë”©: {result['encoding_used']}")
    else:
        print(f"\nâŒ ì˜¤ë¥˜ - {result['file_path']}: {result['error']}")


def main():
    parser = argparse.ArgumentParser(description="HTML íŒŒì¼ì˜ í† í° ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.")
    parser.add_argument("path", help="íŒŒì¼ ë˜ëŠ” ë””ë ‰í† ë¦¬ ê²½ë¡œ")
    parser.add_argument(
        "--model", default="gpt-4", help="ì‚¬ìš©í•  ëª¨ë¸ëª… (ê¸°ë³¸ê°’: gpt-4)"
    )
    parser.add_argument(
        "--extensions",
        nargs="*",
        default=[".html", ".txt", ".md"],
        help="ì²˜ë¦¬í•  íŒŒì¼ í™•ì¥ì (ê¸°ë³¸ê°’: .html .txt .md)",
    )

    args = parser.parse_args()

    path = Path(args.path)

    if path.is_file():
        # ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬
        result = count_tokens_from_file(str(path), args.model)
        print_token_analysis(result)

    elif path.is_directory():
        # ë””ë ‰í† ë¦¬ ì²˜ë¦¬
        results = count_tokens_from_directory(str(path), args.extensions, args.model)

        total_tokens = 0
        success_count = 0

        print(f"\nğŸ“ ë””ë ‰í† ë¦¬: {path}")
        print("=" * 60)

        for result in results:
            print_token_analysis(result)
            if result["status"] == "success":
                total_tokens += result["token_count"]
                success_count += 1

        print("\n" + "=" * 60)
        print("ğŸ“Š ìš”ì•½:")
        print(f"   ì²˜ë¦¬ëœ íŒŒì¼ ìˆ˜: {success_count}")
        print(f"   ì´ í† í° ìˆ˜: {total_tokens:,}")
        print(
            f"   í‰ê·  í† í°/íŒŒì¼: {total_tokens / success_count:,.0f}"
            if success_count > 0
            else "   í‰ê·  í† í°/íŒŒì¼: 0"
        )

    else:
        print(f"âŒ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {path}")


if __name__ == "__main__":
    # ì˜ˆì œ ì‹¤í–‰
    if len(os.sys.argv) == 1:
        # ëª…ë ¹í–‰ ì¸ìê°€ ì—†ëŠ” ê²½ìš° ì˜ˆì œ ì‹¤í–‰
        print("ğŸ” í† í° ì¹´ìš´í„° ì˜ˆì œ ì‹¤í–‰")

        # í˜„ì¬ ë””ë ‰í† ë¦¬ì˜ ì£¼ì„±ì—”ì§€ë‹ˆì–´ë§.html íŒŒì¼ ë¶„ì„
        html_file = "ì£¼ì„±ì—”ì§€ë‹ˆì–´ë§.html"
        if os.path.exists(html_file):
            print(f"\nğŸ“„ {html_file} íŒŒì¼ ë¶„ì„:")
            result = count_tokens_from_file(html_file)
            print_token_analysis(result)
        else:
            print(f"\nâš ï¸  {html_file} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print("í˜„ì¬ ë””ë ‰í† ë¦¬ì˜ HTML íŒŒì¼ë“¤ì„ ì°¾ì•„ì„œ ë¶„ì„í•©ë‹ˆë‹¤...")

            # í˜„ì¬ ë””ë ‰í† ë¦¬ì—ì„œ HTML íŒŒì¼ ì°¾ê¸°
            results = count_tokens_from_directory(".", [".html"])
            if results:
                for result in results:
                    print_token_analysis(result)
            else:
                print("HTML íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        print("\n" + "=" * 60)
        print("ğŸ’¡ ì‚¬ìš©ë²•:")
        print("   python token_counter.py <íŒŒì¼ê²½ë¡œ>          # ë‹¨ì¼ íŒŒì¼ ë¶„ì„")
        print("   python token_counter.py <ë””ë ‰í† ë¦¬ê²½ë¡œ>      # ë””ë ‰í† ë¦¬ ì „ì²´ ë¶„ì„")
        print("   python token_counter.py <ê²½ë¡œ> --model gpt-3.5-turbo  # ëª¨ë¸ ì§€ì •")
    else:
        main()
