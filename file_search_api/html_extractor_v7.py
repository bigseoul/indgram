import os
import re
import sys
from pathlib import Path
from typing import Dict

from bs4 import BeautifulSoup, NavigableString, Tag

# [v7] ê¸°ë³¸ ì„¤ì •
SAMPLE = "hoban/20250403000344.html"

# ì£¼ì„ ë° footnote íŒ¨í„´: (*1), (ì£¼1), *1, [1], ì£¼1 ë“± ëŒ€ì‘
FOOTNOTE_PATTERN = re.compile(
    r"\(\s*[\*ì£¼]?\s*\d+\s*\)|"
    r"\[\s*[\*ì£¼]?\s*\d+\s*\]|"
    r"[\*ì£¼]\d+|"
    r"^\s*[\*ì£¼]\s*$"
)

# [v7] í™•ì¥ëœ íƒœê·¸ ë§¤í•‘ (LLMì´ êµ¬ì¡°ë¥¼ íŒŒì•…í•  ìˆ˜ ìˆëŠ” ìµœì†Œ ë‹¨ìœ„)
TAG_MAP = {
    "table": "t",
    "tr": "r",
    "td": "d",
    "th": "h",
    "div": "v",
    "span": "s",
    "p": "p",
    "h1": "h1",
    "h2": "h2",
    "h3": "h3",
    "h4": "h4",
    "ul": "ul",
    "li": "li",
    "b": "b",
    "strong": "b",
}

# [v7] ì†ì„± ë§¤í•‘ (êµ¬ì¡°ìƒ í•„ìˆ˜ì ì¸ colspan, rowspanë§Œ ë³´ì¡´)
ATTR_MAP = {
    "colspan": "c",
    "rowspan": "r",
}


def _clean_text(text: str, is_navigable_string: bool = False) -> str:
    if not text:
        return ""
    cleaned = FOOTNOTE_PATTERN.sub("", text)
    if not cleaned.strip():
        # ê³µë°± ë…¸ë“œ ë³´ì¡´ ì—¬ë¶€ ê²°ì •
        return " " if text.strip() or is_navigable_string else ""
    # ì—°ì†ëœ ê³µë°± ë° íŠ¹ìˆ˜ ê³µë°±(\xa0) ì •ë¦¬
    cleaned = re.sub(r"[\s\xa0]+", " ", cleaned)
    return cleaned.strip() if not is_navigable_string else cleaned


def _get_simplified_html(tag) -> str:
    """
    íƒœê·¸ì™€ ì†ì„±ì„ ê·¹í•œìœ¼ë¡œ ì••ì¶•í•˜ì—¬ í† í°ì„ ì ˆì•½í•˜ë©´ì„œ êµ¬ì¡° ìœ ì§€
    """
    if isinstance(tag, NavigableString):
        return _clean_text(str(tag), is_navigable_string=True)
    if not isinstance(tag, Tag):
        return ""

    # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì™„ì „ ì œê±°
    if tag.name in ["script", "style", "meta", "link", "noscript", "iframe"]:
        return ""

    # ìì‹ ë…¸ë“œ ì¬ê·€ ì²˜ë¦¬
    inner_parts = []
    for child in tag.children:
        part = _get_simplified_html(child)
        if part:
            inner_parts.append(part)

    inner_html = "".join(inner_parts).strip()

    # ë‚´ìš©ì´ ì—†ëŠ” ê²½ìš° ìŠ¤í‚µ (ë‹¨, í…Œì´ë¸” êµ¬ì¡° íƒœê·¸ëŠ” ë¹ˆ ê°’ì´ë¼ë„ ë³´ì¡´)
    if not inner_html and tag.name not in ["td", "th", "tr", "table"]:
        return ""

    # ì••ì¶•ëœ íƒœê·¸ëª… ê²°ì •
    t_name = TAG_MAP.get(tag.name)

    # ë§¤í•‘ì— ì—†ëŠ” íƒœê·¸ëŠ” êµ¬ì¡°ì  ì˜ë¯¸ê°€ ì ë‹¤ê³  ë³´ê³  í…ìŠ¤íŠ¸ë§Œ ìœ ì§€
    if not t_name:
        return inner_html

    # ì†ì„± ì••ì¶• (c="2" r="3" í˜•íƒœ)
    attrs_str = ""
    for orig_attr, short_attr in ATTR_MAP.items():
        if tag.has_attr(orig_attr):
            attrs_str += f' {short_attr}="{tag[orig_attr]}"'

    return f"<{t_name}{attrs_str}>{inner_html}</{t_name}>"


def _normalize(text: str) -> str:
    if not text:
        return ""
    return re.sub(r"[\s\xa0]+", " ", text).strip()


def _get_company_name(soup: BeautifulSoup) -> str:
    title = soup.title.string if soup.title else ""
    if title:
        match = re.search(
            r"([ê°€-í£a-zA-Z0-9]+?\s*ì£¼ì‹íšŒì‚¬|ì£¼ì‹íšŒì‚¬\s*[ê°€-í£a-zA-Z0-9]+?)", title
        )
        if match:
            return _normalize(match.group(0))
    first_p = soup.find("p")
    if first_p:
        match = re.search(
            r"([ê°€-í£a-zA-Z0-9]+?\s*ì£¼ì‹íšŒì‚¬|ì£¼ì‹íšŒì‚¬\s*[ê°€-í£a-zA-Z0-9]+?)",
            first_p.get_text(),
        )
        if match:
            return _normalize(match.group(0))
    return "Unknown Company"


def _extract_global_meta(soup: BeautifulSoup) -> Dict:
    meta = {"unit": "Unknown", "as_of_date": "Unknown"}
    text = soup.get_text()[:3000]  # ìƒë‹¨ ìœ„ì£¼ ê²€ìƒ‰
    unit_match = re.search(r"\(ë‹¨ìœ„\s*:\s*([ê°€-í£a-z]+)\)", text, re.I)
    if unit_match:
        meta["unit"] = unit_match.group(1)

    date_patterns = [
        r"(\d{4}ë…„\s*\d{1,2}ì›”\s*\d{1,2}ì¼)\s*í˜„ì¬",
        r"ì œ\s*\d+\s*ê¸°ë§\s*(\d{4}ë…„\s*\d{1,2}ì›”\s*\d{1,2}ì¼)",
    ]
    for pattern in date_patterns:
        match = re.search(pattern, text)
        if match:
            meta["as_of_date"] = match.group(1)
            break
    return meta


def extract_full_content_minimized(soup: BeautifulSoup) -> str:
    company_name = _get_company_name(soup)
    meta = _extract_global_meta(soup)

    header = f"[META] C:{company_name} | U:{meta['unit']} | D:{meta['as_of_date']}\n"

    # Body ë‚´ìš© ë˜ëŠ” ì „ì²´ ë‚´ìš© ëŒ€ìƒ
    target = soup.find("body") or soup

    content = _get_simplified_html(target)

    # ìµœì¢… í›„ì²˜ë¦¬: íƒœê·¸ ì‚¬ì´ì˜ ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
    content = re.sub(r"[\s\xa0]+", " ", content)
    content = content.replace("> <", "><")

    # ê°€ë…ì„±ì„ ìœ„í•´ ì£¼ìš” ë¸”ë¡ íƒœê·¸ ë’¤ì— ì¤„ë°”ê¿ˆ ì¶”ê°€
    block_tags = ["t", "r", "p", "h1", "h2", "h3", "h4", "ul", "v"]
    for t in block_tags:
        content = content.replace(f"</{t}>", f"</{t}>\n")

    return header + content


def clear_terminal():
    os.system("cls" if os.name == "nt" else "clear")


if __name__ == "__main__":
    root_path = Path(__file__).resolve().parent.parent
    if str(root_path) not in sys.path:
        sys.path.append(str(root_path))

    try:
        from tokenizer.token_counter import count_tokens_from_text, count_tokens_gemini
    except ImportError:

        def count_tokens_from_text(text, **kwargs):
            return "N/A"

        def count_tokens_gemini(text, **kwargs):
            return "N/A"

    test_file = Path(__file__).resolve().parent / SAMPLE
    if not test_file.exists():
        print(f"Error: {test_file} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    else:
        content = test_file.read_text(encoding="utf-8")
        soup = BeautifulSoup(content, "html.parser")
        result = extract_full_content_minimized(soup)

        # ê²°ê³¼ ì €ì¥
        output_file = test_file.parent / "html_extractor_result_v7_full.txt"
        output_file.write_text(result, encoding="utf-8")

        clear_terminal()
        print("--- [Preivew (First 1500 chars)] ---")
        print(result[:1500] + "\n...")
        print(f"\n[INFO] Full result saved to: {output_file}")

        # í† í° ë¶„ì„
        tokens_gpt = count_tokens_from_text(result, model_name="gpt-4")
        tokens_gemini = count_tokens_gemini(result)

        print("\n" + "=" * 50)
        print("ğŸ“Š Token Analysis (Full Minimized v7):")
        print(f"   Characters: {len(result):,}")
        print(f"   GPT Tokens: {tokens_gpt}")
        print(f"   Gemini Tokens: {tokens_gemini}")
        print("=" * 50)
