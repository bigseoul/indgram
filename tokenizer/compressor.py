"""
HTML Compressor for ChatGPT Token Optimization

HTML ë¬¸ì„œì—ì„œ ì‚¬ëŒì˜ ì´í•´ì— í•„ìš”í•œ ì •ë³´ë§Œ ë‚¨ê¸°ê³ ,
ë‚˜ë¨¸ì§€ ë¶ˆí•„ìš”í•œ ì½”ë“œëŠ” ëª¨ë‘ ì œê±°í•˜ëŠ” íŒŒì´ì¬ ì½”ë“œì…ë‹ˆë‹¤.

ëª©ì :
- ChatGPT APIì— ì „ë‹¬í•  HTML ë°ì´í„°ì˜ í† í° ìˆ˜ë¥¼ ì¤„ì´ê¸° ìœ„í•´
- ì‹œê°ì  ë Œë”ë§ì´ë‚˜ êµ¬ì¡° ìœ ì§€ì—ëŠ” í•„ìš”í•˜ì§€ë§Œ í…ìŠ¤íŠ¸ ì´í•´ì—ëŠ” ë¶ˆí•„ìš”í•œ íƒœê·¸ ë° ë‚´ìš©ì„ ëª¨ë‘ ì œê±°
"""

import os
import re

from bs4 import BeautifulSoup


def compress_html(input_file, output_file):
    """
    HTML íŒŒì¼ì„ ì••ì¶•í•˜ì—¬ í•µì‹¬ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤.

    Args:
        input_file (str): ì…ë ¥ HTML íŒŒì¼ ê²½ë¡œ
        output_file (str): ì¶œë ¥ í…ìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œ
    """

    # HTML íŒŒì¼ ì½ê¸°
    try:
        with open(input_file, "r", encoding="utf-8") as file:
            html_content = file.read()
    except FileNotFoundError:
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_file}")
        return
    except Exception as e:
        print(f"âŒ íŒŒì¼ ì½ê¸° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return

    # BeautifulSoupìœ¼ë¡œ HTML íŒŒì‹±
    soup = BeautifulSoup(html_content, "html.parser")

    # ì œê±°í•  íƒœê·¸ë“¤ - í…ìŠ¤íŠ¸ ì´í•´ì— ë¶ˆí•„ìš”í•œ ìš”ì†Œë“¤
    tags_to_remove = [
        "style",
        "script",
        "meta",
        "link",
        "noscript",
        "iframe",
        "form",
        "input",
        "button",
        "nav",
        "header",
        "footer",
        "aside",
    ]

    # ë¶ˆí•„ìš”í•œ íƒœê·¸ë“¤ ì œê±°
    for tag_name in tags_to_remove:
        for tag in soup.find_all(tag_name):
            tag.decompose()

    # ëª¨ë“  íƒœê·¸ì˜ ì†ì„± ì œê±° (í…ìŠ¤íŠ¸ë§Œ ìœ ì§€)
    for tag in soup.find_all():
        tag.attrs = {}

    # ë¹ˆ íƒœê·¸ë“¤ ì œê±°
    for tag in soup.find_all():
        if not tag.get_text(strip=True) and not tag.find_all():
            tag.decompose()

    # êµ¬ì¡°ì  ìš”ì†Œë“¤ ì²˜ë¦¬ - ì„¹ì…˜ êµ¬ë¶„ì„ ìœ„í•œ ê°œí–‰ ì¶”ê°€
    for tag in soup.find_all(["h1", "h2", "h3", "h4", "h5", "h6"]):
        tag.string = f"\n\n### {tag.get_text().strip()}\n\n"

    # í‘œ ì²˜ë¦¬ - ë” ì½ê¸° ì‰½ê²Œ í¬ë§·íŒ…
    for table in soup.find_all("table"):
        rows = []
        for tr in table.find_all("tr"):
            cells = []
            for td in tr.find_all(["td", "th"]):
                cell_text = td.get_text().strip()
                if cell_text and cell_text != "-":
                    cells.append(cell_text)
            if cells:
                rows.append(" | ".join(cells))

        if rows:
            table_text = "\n".join(rows)
            # í‘œë¥¼ êµ¬ë¶„í•˜ê¸° ìœ„í•œ ê°œí–‰ ì¶”ê°€
            table.replace_with(f"\n\n{table_text}\n\n")

    # ë¬¸ë‹¨ êµ¬ë¶„ì„ ìœ„í•œ ê°œí–‰ ì¶”ê°€
    for p in soup.find_all("p"):
        if p.get_text().strip():
            p.string = f"\n{p.get_text().strip()}\n"

    # í…ìŠ¤íŠ¸ ì¶”ì¶œ
    text_content = soup.get_text()

    # í…ìŠ¤íŠ¸ ì •ë¦¬ - ë¶ˆí•„ìš”í•œ ê³µë°±ê³¼ ì¤„ë°”ê¿ˆ ì œê±°
    # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ ì¤„ì´ê¸° (ì¤„ë°”ê¿ˆì€ ì œì™¸)
    text_content = re.sub(r"[ \t]+", " ", text_content)

    # ì—°ì†ëœ ì¤„ë°”ê¿ˆì„ ìµœëŒ€ 2ê°œê¹Œì§€ë§Œ í—ˆìš©
    text_content = re.sub(r"\n\s*\n\s*\n+", "\n\n", text_content)

    # ì•ë’¤ ê³µë°± ì œê±°
    text_content = text_content.strip()

    # ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥
    try:
        with open(output_file, "w", encoding="utf-8") as file:
            file.write(text_content)
        print(f"âœ… ì••ì¶• ì™„ë£Œ: {output_file}")
        print(f"ğŸ“Š ì›ë³¸ íŒŒì¼ í¬ê¸°: {os.path.getsize(input_file):,} bytes")
        print(f"ğŸ“Š ì••ì¶• íŒŒì¼ í¬ê¸°: {os.path.getsize(output_file):,} bytes")

        # ì••ì¶•ë¥  ê³„ì‚°
        compression_ratio = (
            1 - os.path.getsize(output_file) / os.path.getsize(input_file)
        ) * 100
        print(f"ğŸ“ˆ ì••ì¶•ë¥ : {compression_ratio:.1f}%")

        # í† í° ìˆ˜ ì¶”ì • (í•œê¸€ ê¸°ì¤€ ëŒ€ëµì  ê³„ì‚°)
        estimated_tokens = len(text_content.split())
        print(f"ğŸ”¢ ì¶”ì • í† í° ìˆ˜: {estimated_tokens:,} ê°œ")

    except Exception as e:
        print(f"âŒ íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")


def process_files_folder():
    """source í´ë”ì˜ HTML íŒŒì¼ì„ target í´ë”ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥í•©ë‹ˆë‹¤."""
    # í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ê°€ tokenizer í´ë”ì— ìˆìœ¼ë¯€ë¡œ, before/after í´ë”ëŠ” ê°™ì€ ë ˆë²¨ì— ìˆìŠµë‹ˆë‹¤
    script_dir = os.path.dirname(os.path.abspath(__file__))
    source_dir = os.path.join(script_dir, "before")
    target_dir = os.path.join(script_dir, "cleaned")

    if not os.path.exists(source_dir):
        print(f"âŒ {source_dir} í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    # target í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±
    os.makedirs(target_dir, exist_ok=True)

    # HTML íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° (source í´ë” ë‚´ .html)
    html_files = [f for f in os.listdir(source_dir) if f.endswith(".html")]

    if not html_files:
        print(f"âŒ {source_dir} í´ë”ì—ì„œ HTML íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    print("ğŸš€ HTML íŒŒì¼ ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘")
    print(f"ğŸ“¥ ì…ë ¥ í´ë”: {source_dir}")
    print(f"ğŸ“¤ ì¶œë ¥ í´ë”: {target_dir}")
    print(f"ğŸ“„ ë°œê²¬ëœ HTML íŒŒì¼: {len(html_files)}ê°œ")
    print("-" * 50)

    processed_count = 0
    for html_file in html_files:
        # ì…ë ¥ íŒŒì¼ ê²½ë¡œ
        input_path = os.path.join(source_dir, html_file)

        # ì¶œë ¥ íŒŒì¼ëª… ìƒì„± (í™•ì¥ì ì œê±° í›„ _í›„ì²˜ë¦¬.txt ì¶”ê°€)
        file_name_without_ext = os.path.splitext(html_file)[0]
        output_filename = f"{file_name_without_ext}_í›„ì²˜ë¦¬.txt"
        output_path = os.path.join(target_dir, output_filename)

        print(f"\nğŸ“ ì²˜ë¦¬ ì¤‘: {html_file}")
        compress_html(input_path, output_path)
        processed_count += 1

    print("\nâœ… ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ!")
    print(f"ğŸ¯ ì´ ì²˜ë¦¬ëœ íŒŒì¼: {processed_count}ê°œ")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # files í´ë”ì˜ ëª¨ë“  HTML íŒŒì¼ ì²˜ë¦¬
    process_files_folder()


if __name__ == "__main__":
    main()
