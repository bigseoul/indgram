import re
import sys
from pathlib import Path
from typing import Dict

from bs4 import BeautifulSoup

SAMPLE = "íˆ¬ë¯¹ìŠ¤í™€ë”©ìŠ¤.html"

# ì£¼ì„ íŒ¨í„´: (*1), (ì£¼1), *1, (1) ë“± ëŒ€ì‘
FOOTNOTE_PATTERN = re.compile(r"\(\s*[\*ì£¼]?\s*\d+\s*\)|\*?\d+")


def _clean_text(text: str) -> str:
    """í…ìŠ¤íŠ¸ ë‚´ì˜ ì§€ì €ë¶„í•œ ì£¼ì„ ë§ˆì»¤ ì œê±°"""
    if not text:
        return ""
    # í…ìŠ¤íŠ¸ê°€ ì£¼ì„ ë§ˆì»¤ë¡œë§Œ ì´ë£¨ì–´ì ¸ ìˆê±°ë‚˜ ë§ˆì»¤ë¡œ ì‹œì‘í•˜ëŠ” ì„¤ëª…ë¬¸ì´ë©´ ë¹ˆê°’ ë°˜í™˜
    if re.match(r"^\s*[\(\{\[]?[\*ì£¼]?\d+[\)\}\]]?[\s\.]*$", text):
        return ""
    # ë³¸ë¬¸ ë‚´ì˜ (ì£¼1), (*1) ë“± íŒ¨í„´ ì‚­ì œ
    return re.sub(r"\(\s*[\*ì£¼]?\s*\d+\s*\)", "", text).strip()


def _get_simplified_html(tag) -> str:
    """
    íƒœê·¸ì˜ ëª¨ë“  ì†ì„±(style, class ë“±)ì„ ì œê±°í•˜ê³ 
    í•µì‹¬ êµ¬ì¡°ì™€ í…ìŠ¤íŠ¸ë§Œ ë‚¨ê¸´ HTML ë¬¸ìì—´ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    from bs4 import NavigableString, Tag

    if isinstance(tag, NavigableString):
        return _clean_text(str(tag))

    if not isinstance(tag, Tag):
        return ""

    # ì£¼ì„ìœ¼ë¡œë§Œ êµ¬ì„±ëœ íƒœê·¸ëŠ” í†µì§¸ë¡œ ìŠ¤í‚µ
    raw_tag_text = tag.get_text().strip()
    if raw_tag_text and not _clean_text(raw_tag_text):
        return ""

    # ìì‹ ë…¸ë“œë“¤ì„ ë¨¼ì € ë‹¨ìˆœí™”
    inner_html = "".join(_get_simplified_html(child) for child in tag.children).strip()

    # ë‚´ìš©ì´ ì—†ëŠ” ë¹„ë³¸ì§ˆì ì¸ íƒœê·¸ëŠ” ì œê±° (ë‹¨, í…Œì´ë¸” ì…€ì€ êµ¬ì¡°ìƒ ìœ ì§€)
    if not inner_html and tag.name not in ["td", "th", "tr"]:
        return ""

    # í…Œì´ë¸” êµ¬ì¡°ë¥¼ ìœ„í•œ ì†ì„±(colspan, rowspan)ì€ ë³´ì¡´
    attrs_str = ""
    if tag.name in ["td", "th"]:
        for attr in ["colspan", "rowspan"]:
            if tag.has_attr(attr):
                attrs_str += f' {attr}="{tag[attr]}"'

    return f"<{tag.name}{attrs_str}>{inner_html}</{tag.name}>"


def _normalize(text: str) -> str:
    """
    í…ìŠ¤íŠ¸ ì •ê·œí™”: ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
    """
    if not text:
        return ""
    return re.sub(r"[\s\xa0]+", " ", text).strip()


def _get_company_name(soup: BeautifulSoup) -> str:
    """
    ë¬¸ì„œì—ì„œ íšŒì‚¬ëª… ì¶”ì¶œ
    """
    title = soup.title.string if soup.title else ""
    if title:
        match = re.search(
            r"([ê°€-í£a-zA-Z0-9]+?\s*ì£¼ì‹íšŒì‚¬|ì£¼ì‹íšŒì‚¬\s*[ê°€-í£a-zA-Z0-9]+?)", title
        )
        if match:
            return _normalize(match.group(0))

    first_p = soup.find("p")
    if first_p:
        text = first_p.get_text()
        match = re.search(
            r"([ê°€-í£a-zA-Z0-9]+?\s*ì£¼ì‹íšŒì‚¬|ì£¼ì‹íšŒì‚¬\s*[ê°€-í£a-zA-Z0-9]+?)", text
        )
        if match:
            return _normalize(match.group(0))

    return "Unknown Company"


def _extract_global_meta(soup: BeautifulSoup) -> Dict:
    """
    ì „ì—­ ë©”íƒ€ë°ì´í„°(ë‹¨ìœ„, ê¸°ì¤€ì¼) ì¶”ì¶œ
    """
    meta = {"unit": "ì›", "as_of_date": "Unknown"}
    text = soup.get_text()

    unit_match = re.search(r"\(ë‹¨ìœ„\s*:\s*([ê°€-í£]+)\)", text)
    if unit_match:
        meta["unit"] = unit_match.group(1)

    date_patterns = [
        r"(\d{4}ë…„\s*\d{1,2}ì›”\s*\d{1,2}ì¼)\s*í˜„ì¬",
        r"ì œ\s*\d+\s*ê¸°ë§\s*(\d{4}ë…„\s*\d{1,2}ì›”\s*\d{1,2}ì¼)",
    ]
    for pattern in date_patterns:
        match = re.search(pattern, text)
        if match:
            meta["as_of_date"] = match.group(1)
            break

    return meta


def extract_evidence_blocks(soup: BeautifulSoup) -> str:
    """
    HTML ì›ë³¸ ì¶”ì¶œê¸°:
    1. ì¼ë°˜ ì‚¬í•­ í‚¤ì›Œë“œ: ë¬¸ë§¥ ìœ ì§€ (íƒœê·¸ ì›ë³¸ í¬í•¨)
    2. ë°ì´í„° í‚¤ì›Œë“œ: ì˜¤ì§ 'ì§€ë¶„ìœ¨' í…Œì´ë¸”ë§Œ ì›ë³¸ìœ¼ë¡œ ì¶”ì¶œ
    """
    company_name = _get_company_name(soup)
    global_meta = _extract_global_meta(soup)

    # ì„¹ì…˜ í—¤ë” ì •ê·œì‹: "1. íšŒì‚¬ì˜ ê°œìš”", "I. ì¼ë°˜ì‚¬í•­", "1.ì¼ë°˜ì‚¬í•­" ë“± ëŒ€ì‘
    section_pattern = re.compile(
        r"^\s*([0-9]{1,2}|[IVX]{1,3}|[ê°€-í•˜])[\.\)\s]+\s*(íšŒì‚¬ì˜\s*ê°œìš”|ì¼ë°˜ì‚¬í•­|ì¼ë°˜ì ì¸\s*ì‚¬í•­|ì¼ë°˜\s*ì‚¬í•­)",
        re.IGNORECASE,
    )
    # ëª¨ë“  ìƒìœ„ ì„¹ì…˜ ë²ˆí˜¸/ì œëª©ì„ ì¶”ì í•˜ê¸° ìœ„í•œ ì •ê·œì‹
    any_section_pattern = re.compile(
        r"^\s*([0-9]{1,2}|[IVX]{1,3}|[ê°€-í•˜])[\.\)\s]+\s*([ê°€-í£\s]{2,50})",
    )
    # ì§€ë¶„ êµ¬ì¡°ì™€ ì§ì ‘ ê´€ë ¨ëœ í™•ì‹¤í•œ í‚¤ì›Œë“œ ì¡°í•© (ë²”ìš©ì„± ê³ ë ¤)
    data_keywords = [
        "ì§€ë¶„ìœ¨",
        "ì£¼ì£¼",
        "ìë³¸ê¸ˆ",
        "ì¶œì",
        "ì¢…ì†ê¸°ì—…",
        "í”¼íˆ¬ì",
        "ì†Œìœ ",
        "ë³´ìœ ",
        "ì§€ë°°",
    ]
    term_markers = ["ë‹¹ê¸°", "ë‹¹ê¸°ë§", "ë‹¹ê¸° ë§", "í˜„ì¬"]
    # ë…¸ì´ì¦ˆ(ê±°ë˜, ì±„ê¶Œ/ì±„ë¬´, ë‹´ë³´ ë“±)ë¥¼ ê±¸ëŸ¬ë‚´ê¸° ìœ„í•œ ì œì™¸ í‚¤ì›Œë“œ ê°•í™”
    exclude_keywords = [
        "ë¹„ì§€ë°°ì§€ë¶„ìœ¨",
        "ë¹„ì§€ë°°ì§€ë¶„",
        "ì±„ê¶Œ",
        "ì±„ë¬´",
        "ë§¤ì¶œ",
        "ë§¤ì…",
        "ì§€ê¸‰ë³´ì¦",
        "ë‹´ë³´ì œê³µ",
        "ì£¼ìš”ê±°ë˜",
        "ìê¸ˆê±°ë˜",
        "ìˆ˜ìµ",
        "ë¹„ìš©",
        "ì±„ë¬´ë©´ì œ",
    ]

    header = [
        "[META]",
        f"Company: {company_name}",
        f"Unit: {global_meta.get('unit', 'Unknown')}",
        f"Date: {global_meta.get('as_of_date', 'Unknown')}",
    ]

    evidence = ["\n".join(header)]
    seen_elements = set()

    # ëª¨ë“  ìš”ì†Œë¥¼ ìˆœì°¨ì ìœ¼ë¡œ íƒìƒ‰
    all_tags = soup.find_all(["h1", "h2", "h3", "p", "table", "div", "span"])

    idx = 0
    current_section = "Unknown Section"
    while idx < len(all_tags):
        tag = all_tags[idx]
        if tag in seen_elements:
            idx += 1
            continue

        raw_text = tag.get_text().strip()
        if not raw_text:
            idx += 1
            continue

        # í˜„ì¬ ìœ„ì¹˜í•œ ì„¹ì…˜ ì œëª© ì—…ë°ì´íŠ¸ (ê¸¸ì´ê°€ ì§§ê³  íŠ¹ì • ì¡°ê±´(H1-H3 ë˜ëŠ” ì§§ì€ P)ì„ ë§Œì¡±í•  ë•Œë§Œ)
        if tag.name in ["h1", "h2", "h3"] or (tag.name == "p" and len(raw_text) < 100):
            # ì¤„ë°”ê¿ˆì´ ìˆëŠ” ê²½ìš° ì²« ì¤„ë§Œ ì‚¬ìš© (ì„¹ì…˜ ì œëª©ì´ ì—¬ëŸ¬ ì¤„ì¼ ë¦¬ ì—†ìœ¼ë¯€ë¡œ)
            first_line = raw_text.split("\n")[0].strip()
            section_m = any_section_pattern.match(first_line)
            if section_m:
                current_section = first_line

        # ì£¼ì„ ë§ˆì»¤ë§Œ ìˆëŠ” íƒœê·¸ì´ê±°ë‚˜ ì£¼ì„ ì„¤ëª…ë¬¸(ì˜ˆ: *1 ...)ì´ë©´ ê±´ë„ˆëœ€
        if not _clean_text(raw_text) or re.match(r"^\s*[\(\{\[]?[\*ì£¼]\d+", raw_text):
            seen_elements.add(tag)
            idx += 1
            continue

        # 1. ì¼ë°˜ì ì¸ ì‚¬í•­: ì •ê·œì‹ìœ¼ë¡œ ì„¹ì…˜ ì‹œì‘ì  í¬ì°©
        if section_pattern.match(raw_text):
            block_content = []
            curr_idx = idx
            count = 0
            while curr_idx < len(all_tags) and count < 15:
                t = all_tags[curr_idx]
                # ë‹¤ë¥¸ ëŒ€ë‹¨ì›ì´ ì‹œì‘ë˜ë©´ ì¤‘ë‹¨
                if count > 0 and (
                    t.name in ["h1", "h2", "h3"]
                    or section_pattern.match(t.get_text().strip())
                ):
                    break
                if t not in seen_elements:
                    # ì£¼ì„ íŒ¨í„´ ì¬í™•ì¸
                    if not re.match(r"^\s*[\(\{\[]?[\*ì£¼]\d+", t.get_text().strip()):
                        block_content.append(_get_simplified_html(t))
                        seen_elements.add(t)
                        # ìì‹ ìš”ì†Œë“¤ë„ seen_elementsì— ì¶”ê°€
                        if hasattr(t, "find_all"):
                            for desc in t.find_all(True):
                                seen_elements.add(desc)
                curr_idx += 1
                count += 1
            if block_content:
                # ì„¹ì…˜ ì •ë³´ë¥¼ ëª…ì‹œì ìœ¼ë¡œ í¬í•¨
                evidence.append(
                    f"[DATA-GENERAL-HTML]\n[Section: {current_section}]\n"
                    + "\n".join(block_content)
                )
            idx = curr_idx
            continue

        # 2. 'ì§€ë¶„ìœ¨' í‚¤ì›Œë“œ: í…Œì´ë¸” ìœ„ì£¼ë¡œ íƒìƒ‰
        elif any(kw in raw_text for kw in data_keywords):
            # ì•µì»¤ íƒœê·¸ê°€ ë„ˆë¬´ ê¸¸ë©´ ë¬´ì‹œí•˜ë˜, divë‚˜ tableì€ ë‚´ë¶€ ê²€ìƒ‰ì„ ìœ„í•´ í—ˆìš©
            if len(raw_text) > 500 and tag.name not in ["table", "div"]:
                idx += 1
                continue

            target_table = None
            if tag.name == "table":
                target_table = tag
            else:
                # ë‹¤ìŒ 10ê°œ ë…¸ë“œ ë‚´ì—ì„œ í…Œì´ë¸” íƒìƒ‰
                search_idx = idx + 1
                for _ in range(10):
                    if search_idx >= len(all_tags):
                        break
                    if all_tags[search_idx].name == "table":
                        target_table = all_tags[search_idx]
                        break
                    search_idx += 1

            if target_table and target_table not in seen_elements:
                table_text = target_table.get_text()

                # 'ì§€ë¶„ìœ¨' ë‹¨ì–´ê°€ í…Œì´ë¸” ë‚´ë¶€ì— ëª…ì‹œì ìœ¼ë¡œ ìˆì–´ì•¼ í•¨
                if "ì§€ë¶„ìœ¨" in table_text and not any(
                    ek in table_text for ek in exclude_keywords
                ):
                    # ì‹œì  íŒë³„ ë° ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘: í…Œì´ë¸” ìœ„ìª½ 10ê°œ ë…¸ë“œ ë‚´ì— ì‹œì  ë§ˆì»¤ê°€ ìˆëŠ”ì§€ í™•ì¸
                    term_context_tags = []
                    term_context_text = table_text
                    search_curr = target_table
                    for _ in range(10):
                        p_node = search_curr.find_previous_sibling()
                        if not p_node:
                            break
                        if p_node.name in ["h1", "h2", "h3"]:
                            break

                        node_text = p_node.get_text().strip()
                        term_context_text += node_text

                        # (ë‹¹ê¸°ë§), (ì „ê¸°ë§), (ë‹¨ìœ„:) ë“±ì„ í¬í•¨í•œ ë©”íƒ€ë°ì´í„°ì„± íƒœê·¸ í˜¹ì€ ì„¤ëª… ë‹¨ë½ ì¶”ê°€
                        is_marker = any(
                            m in node_text for m in term_markers + ["(ë‹¨ìœ„"]
                        )
                        is_metadata_node = p_node.name == "table" or (
                            p_node.name == "p" and len(node_text) < 200
                        )

                        if is_marker or is_metadata_node:
                            term_context_tags.insert(0, _get_simplified_html(p_node))
                        search_curr = p_node

                    # í…Œì´ë¸” í—¤ë” ìì²´ì—ì„œë„ ì‹œì  ë§ˆì»¤ í™•ì¸ (ìƒìœ„ 3ì¤„ê¹Œì§€ í™•ì¸)
                    header_text = (
                        target_table.thead.get_text() if target_table.thead else ""
                    )
                    if not header_text:
                        # theadê°€ ì—†ëŠ” ê²½ìš° ìƒìœ„ 3ê°œì˜ trì„ í•©ì³ì„œ í™•ì¸
                        header_rows = target_table.find_all("tr")[:3]
                        header_text = " ".join(r.get_text() for r in header_rows)

                    if any(m in term_context_text for m in term_markers) or any(
                        m in header_text for m in term_markers
                    ):
                        # Anchor í…ìŠ¤íŠ¸ ì •ì œ
                        clean_anchor = re.sub(r"\s+", " ", raw_text[:150]).strip()
                        title_info = f"<p><b>[Anchor]</b> {clean_anchor}</p>"
                        context_html = "\n".join(term_context_tags)
                        evidence.append(
                            f"[DATA-TABLE-HTML]\n[Section: {current_section}]\n{title_info}\n{context_html}\n{_get_simplified_html(target_table)}"
                        )

                        # íƒ€ê²Ÿ í…Œì´ë¸”ê³¼ ì•µì»¤ íƒœê·¸, ê·¸ë¦¬ê³  ê·¸ ìì‹ë“¤ì„ ëª¨ë‘ seen ì²˜ë¦¬
                        seen_elements.add(target_table)
                        for desc in target_table.find_all(True):
                            seen_elements.add(desc)
                        seen_elements.add(tag)
                        for desc in tag.find_all(True):
                            seen_elements.add(desc)
                else:
                    # í…Œì´ë¸” ë‚´ë¶€ì—ëŠ” í‚¤ì›Œë“œê°€ ì—†ì§€ë§Œ í…ìŠ¤íŠ¸ ë¸”ë¡ ìì²´ì— ì£¼ì‹ ê´€ë ¨ í•µì‹¬ ì •ë³´ê°€ ìˆëŠ” ê²½ìš°
                    # ë‹¨ìˆœíˆ 'ê´€ê³„'ë‚˜ 'ë‹¹ê¸°ë§'ë§Œ ìˆë‹¤ê³  ê°€ì ¸ì˜¤ì§€ ì•Šê³ , 'ì£¼ì£¼'ë‚˜ 'ì§€ë¶„'ì´ ëª…ì‹œë˜ì–´ì•¼ í•¨
                    if any(m in raw_text for m in term_markers) and any(
                        k in raw_text for k in ["ì§€ë¶„", "ì£¼ì£¼", "ë³´ìœ "]
                    ):
                        evidence.append(
                            f"[DATA-GENERAL-HTML]\n[Section: {current_section}]\n{_get_simplified_html(tag)}"
                        )
                        seen_elements.add(tag)
            else:
                # ë‹¤ìŒ 10ê°œ ë…¸ë“œ ë‚´ì— í…Œì´ë¸”ì´ ì—†ë”ë¼ë„, í…ìŠ¤íŠ¸ ë¸”ë¡ ìì²´ì— í•µì‹¬ ì •ë³´ê°€ ìˆìœ¼ë©´ ì¶”ì¶œ
                if any(m in raw_text for m in term_markers) and any(
                    k in raw_text for k in ["ì§€ë¶„", "ì£¼ì£¼", "ë³´ìœ "]
                ):
                    evidence.append(
                        f"[DATA-GENERAL-HTML]\n[Section: {current_section}]\n{_get_simplified_html(tag)}"
                    )
                    seen_elements.add(tag)

        idx += 1

    return "\n\n---\n\n".join(evidence)


if __name__ == "__main__":
    # í…ìŠ¤íŠ¸ ì¹´ìš´í„° ëª¨ë“ˆ ê²½ë¡œ ì¶”ê°€
    sys.path.append(str(Path(__file__).resolve().parent.parent.parent / "tokenizer"))
    try:
        from token_counter import count_tokens_from_text, count_tokens_gemini
    except ImportError:
        count_tokens_from_text = None
        count_tokens_gemini = None

    test_file = Path(__file__).resolve().parent / "sample" / SAMPLE
    if not test_file.exists():
        print(f"Error: {test_file} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    else:
        content = test_file.read_text(encoding="utf-8")
        soup = BeautifulSoup(content, "html.parser")
        result = extract_evidence_blocks(soup)
        print(result)

        if count_tokens_from_text:
            tokens_gpt = count_tokens_from_text(result, model_name="gpt-5-nano")
            tokens_gemini = (
                count_tokens_gemini(result) if count_tokens_gemini else "N/A"
            )

            print("\n" + "=" * 50)
            print("ğŸ“Š Token Analysis (Extracted Content):")
            print(f"   Characters: {len(result):,}")
            print(f"   GPT Tokens: {tokens_gpt:,}")
            print(
                f"   Gemini Tokens: {tokens_gemini if isinstance(tokens_gemini, str) else f'{tokens_gemini:,}'}"
            )
            print("=" * 50)
