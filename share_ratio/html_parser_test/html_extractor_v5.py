import os
import re
import sys
from pathlib import Path
from typing import Dict, List

from bs4 import BeautifulSoup, NavigableString, Tag

SAMPLE = "íˆ¬ë¯¹ìŠ¤í™€ë”©ìŠ¤.html"

# ì£¼ì„ ë° footnote íŒ¨í„´: (*1), (ì£¼1), *1, [1], ì£¼1 ë“± ëŒ€ì‘
FOOTNOTE_PATTERN = re.compile(
    r"\(\s*[\*ì£¼]?\s*\d+\s*\)|"  # (1), (*1), (ì£¼1)
    r"\[\s*[\*ì£¼]?\s*\d+\s*\]|"  # [1], [*1]
    r"[\*ì£¼]\d+|"  # *1, ì£¼1
    r"^\s*[\*ì£¼]\s*$"  # ë‹¨ë… * ë˜ëŠ” ì£¼
)

# [v5] ì§€ë¶„ìœ¨ ë°ì´í„° ì‹ë³„ íŒ¨í„´: % ë˜ëŠ” "ì§€ë¶„ìœ¨" í…ìŠ¤íŠ¸
RATIO_PATTERN = re.compile(r"%|ì§€ë¶„ìœ¨")


def clear_terminal():
    os.system("cls" if os.name == "nt" else "clear")


def _clean_text(text: str, is_navigable_string: bool = False) -> str:
    if not text:
        return ""
    cleaned = FOOTNOTE_PATTERN.sub("", text)
    if not cleaned.strip():
        return " " if text.strip() or is_navigable_string else ""
    if is_navigable_string:
        return re.sub(r"\s+", " ", cleaned)
    return cleaned.strip()


def _get_simplified_html(tag) -> str:
    if isinstance(tag, NavigableString):
        return _clean_text(str(tag), is_navigable_string=True)
    if not isinstance(tag, Tag):
        return ""
    raw_tag_text = tag.get_text()
    if raw_tag_text.strip() and not _clean_text(raw_tag_text).strip():
        return ""
    inner_html = "".join(_get_simplified_html(child) for child in tag.children)
    inner_html = re.sub(r"\s+", " ", inner_html).strip()
    if not inner_html and tag.name not in ["td", "th", "tr"]:
        return ""
    attrs_str = ""
    if tag.name in ["td", "th"]:
        for attr in ["colspan", "rowspan"]:
            if tag.has_attr(attr):
                attrs_str += f' {attr}="{tag[attr]}"'
    return f"<{tag.name}{attrs_str}>{inner_html}</{tag.name}>"


def _normalize(text: str) -> str:
    if not text:
        return ""
    return re.sub(r"[\s\xa0]+", " ", text).strip()


def _get_company_name(soup: BeautifulSoup) -> str:
    title = soup.title.string if soup.title else ""
    if title:
        match = re.search(
            r"([ê°€-í£a-zA-Z0-9]+?\s*ì£¼ì‹íšŒì‚¬|ì£¼ì‹íšŒì‚¬\s*[ê°€-í£a-zA-Z0-9]+?)", title
        )
        if match:
            return _normalize(match.group(0))
    first_p = soup.find("p")
    if first_p:
        text = first_p.get_text()
        match = re.search(
            r"([ê°€-í£a-zA-Z0-9]+?\s*ì£¼ì‹íšŒì‚¬|ì£¼ì‹íšŒì‚¬\s*[ê°€-í£a-zA-Z0-9]+?)", text
        )
        if match:
            return _normalize(match.group(0))
    return "Unknown Company"


def _extract_global_meta(soup: BeautifulSoup) -> Dict:
    meta = {"unit": "ì›", "as_of_date": "Unknown"}
    text = soup.get_text()
    unit_match = re.search(r"\(ë‹¨ìœ„\s*:\s*([ê°€-í£]+)\)", text)
    if unit_match:
        meta["unit"] = unit_match.group(1)
    date_patterns = [
        r"(\d{4}ë…„\s*\d{1,2}ì›”\s*\d{1,2}ì¼)\s*í˜„ì¬",
        r"ì œ\s*\d+\s*ê¸°ë§\s*(\d{4}ë…„\s*\d{1,2}ì›”\s*\d{1,2}ì¼)",
    ]
    for pattern in date_patterns:
        match = re.search(pattern, text)
        if match:
            meta["as_of_date"] = match.group(1)
            break
    return meta


def _get_preceding_context(tag: Tag, max_tags: int = 2) -> List[Tag]:
    """í…Œì´ë¸” ì•ì˜ ì„¤ëª… íƒœê·¸(p, div ë“±)ë¥¼ ìµœëŒ€ max_tagsê°œê¹Œì§€ ê°€ì ¸ì˜´"""
    context_tags = []
    prev = tag.find_previous_sibling()
    count = 0
    while prev and count < max_tags:
        if isinstance(prev, Tag) and prev.name in ["p", "div", "span"]:
            text = prev.get_text().strip()
            # ë¹ˆ íƒœê·¸ê°€ ì•„ë‹ˆê³ , ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸(100ì ì´ˆê³¼)ê°€ ì•„ë‹ˆë©´ ì¶”ê°€
            if text and len(text) < 150:
                context_tags.insert(0, prev)
                count += 1
        prev = prev.find_previous_sibling()
    return context_tags


def _get_section_title(tag: Tag) -> str:
    """í˜„ì¬ íƒœê·¸ê°€ ì†í•œ ì„¹ì…˜ì˜ ì œëª©ì„ ì°¾ìŒ"""
    section_pattern = re.compile(
        r"^\s*([0-9]{1,2}|[IVX]{1,3}|[ê°€-í•˜])[\.)\s]+\s*([ê°€-í£\s]{2,50})"
    )
    # ìœ„ë¡œ ì˜¬ë¼ê°€ë©° ì„¹ì…˜ ì œëª© ì°¾ê¸°
    prev = tag
    for _ in range(30):  # ìµœëŒ€ 30ê°œ íƒœê·¸ê¹Œì§€ íƒìƒ‰
        prev = prev.find_previous()
        if not prev:
            break
        if isinstance(prev, Tag):
            text = prev.get_text().strip()
            if len(text) < 100:
                match = section_pattern.match(text.split("\n")[0])
                if match:
                    return text.split("\n")[0].strip()
    return "Unknown Section"


def extract_evidence_blocks(soup: BeautifulSoup) -> str:
    """
    HTML ì›ë³¸ ì¶”ì¶œê¸° (v5):
    - í•µì‹¬ ì›ì¹™: '%' ë˜ëŠ” 'ì§€ë¶„ìœ¨' í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ë¬´ì¡°ê±´ ì¶”ì¶œ
    - í…Œì´ë¸” ì¶”ì¶œ ì‹œ ë°”ë¡œ ì•ì˜ ì„¤ëª… íƒœê·¸ë„ í•¨ê»˜ ì¶”ì¶œ
    - ì„¹ì…˜/í‚¤ì›Œë“œ í•„í„°ë§ ìµœì†Œí™”
    """
    company_name = _get_company_name(soup)
    global_meta = _extract_global_meta(soup)

    header = [
        "[META]",
        f"Company: {company_name}",
        f"Unit: {global_meta.get('unit', 'Unknown')}",
        f"Date: {global_meta.get('as_of_date', 'Unknown')}",
    ]

    evidence = ["\n".join(header)]
    seen_elements = set()

    # [v5] ëª¨ë“  í…Œì´ë¸”ê³¼ í…ìŠ¤íŠ¸ ë¸”ë¡ì„ ìˆœíšŒ
    all_tags = soup.find_all(["table", "p", "div", "span"])

    for tag in all_tags:
        if tag in seen_elements:
            continue

        raw_text = tag.get_text()
        if not raw_text.strip():
            continue

        # [v5] í•µì‹¬ ì¡°ê±´: % ë˜ëŠ” ì§€ë¶„ìœ¨ì´ ìˆìœ¼ë©´ ì¶”ì¶œ
        has_ratio = bool(RATIO_PATTERN.search(raw_text))

        if not has_ratio:
            seen_elements.add(tag)
            continue

        section_title = _get_section_title(tag)

        # í…Œì´ë¸” ì²˜ë¦¬
        if tag.name == "table":
            # í…Œì´ë¸” ì•ì˜ ì„¤ëª… ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ
            context_tags = _get_preceding_context(tag)
            context_html = ""
            for ctx_tag in context_tags:
                if ctx_tag not in seen_elements:
                    ctx_html = _get_simplified_html(ctx_tag)
                    if ctx_html.strip():
                        context_html += ctx_html + "\n"
                    seen_elements.add(ctx_tag)

            table_html = _get_simplified_html(tag)
            block = f"[DATA-TABLE-HTML]\n[Section: {section_title}]\n"
            if context_html.strip():
                block += f"[Context]\n{context_html.strip()}\n[Table]\n"
            block += table_html

            evidence.append(block)
            seen_elements.add(tag)
            for desc in tag.find_all(True):
                seen_elements.add(desc)

        # í…ìŠ¤íŠ¸ ë¸”ë¡ ì²˜ë¦¬ (p, div, span)
        else:
            simplified = _get_simplified_html(tag)
            if simplified.strip():
                evidence.append(
                    f"[DATA-GENERAL-HTML]\n[Section: {section_title}]\n{simplified}"
                )
            seen_elements.add(tag)
            # ìì‹ íƒœê·¸ë„ seenì— ì¶”ê°€í•˜ì—¬ ì¤‘ë³µ ë°©ì§€
            if hasattr(tag, "find_all"):
                for desc in tag.find_all(True):
                    seen_elements.add(desc)

    return "\n\n---\n\n".join(evidence)


if __name__ == "__main__":
    sys.path.append(str(Path(__file__).resolve().parent.parent.parent / "tokenizer"))
    try:
        from token_counter import count_tokens_from_text, count_tokens_gemini
    except ImportError:
        count_tokens_from_text = None
        count_tokens_gemini = None

    test_file = Path(__file__).resolve().parent / "sample" / SAMPLE
    if not test_file.exists():
        print(f"Error: {test_file} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    else:
        content = test_file.read_text(encoding="utf-8")
        soup = BeautifulSoup(content, "html.parser")
        result = extract_evidence_blocks(soup)
        clear_terminal()
        print(result)

        # ê²°ê³¼ íŒŒì¼ ì €ì¥
        output_file = test_file.parent / "html_extractor_result.html"
        output_file.write_text(result, encoding="utf-8")
        print(f"\n[INFO] Result saved to: {output_file}")

        if count_tokens_from_text:
            tokens_gpt = count_tokens_from_text(result, model_name="gpt-5-nano")
            tokens_gemini = (
                count_tokens_gemini(result) if count_tokens_gemini else "N/A"
            )
            print("\n" + "=" * 50)
            print("ğŸ“Š Token Analysis (Extracted Content):")
            print(f"   Characters: {len(result):,}")
            print(f"   GPT Tokens: {tokens_gpt:,}")
            print(
                f"   Gemini Tokens: {tokens_gemini if isinstance(tokens_gemini, str) else f'{tokens_gemini:,}'}"
            )
            print("=" * 50)
